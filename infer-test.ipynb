{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Kaggle GI Tract Inference\n\nUsing code snippets from:\nhttps://www.kaggle.com/code/yiheng/3d-solution-with-monai-produce-3d-data/notebook\nhttps://www.kaggle.com/code/israrahmed919/createmasksopencv\nhttps://www.kaggle.com/code/clemchris/gi-seg-pytorch-train-infer\n\n\n","metadata":{"papermill":{"duration":0.075462,"end_time":"2022-05-16T10:43:20.981298","exception":false,"start_time":"2022-05-16T10:43:20.905836","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**Notes**\n* See this post for how to submit.  https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/discussion/320541#1764874  The order of the submission has to match the sample_submission.csv file that appears during Kaggle's evaluation\n","metadata":{}},{"cell_type":"markdown","source":"### Submission packages\nSubmissions need to run with the internet connectivity off so we need to use Kaggle Datasets to hold any packages that are not in the standard Kaggle image.\n\nExample:\n\n1. On a local machine download the package\n\n    `pip download einops -d ./einops/`\n\n    `pip download segmentation-models-pytorch -d ./segmentation-models-pytorch/`\n\n2. Pick through the packages and select the parts you need\n\n3. add the files to a Kaggle dataset\n\n4. Add the dataset to your notebook\n\n5. Import packages from files\n\n    `!pip install '../input/einops041/einops-0.4.1-py3-none-any.whl'`","metadata":{}},{"cell_type":"markdown","source":"Note - it looks like the below does not work in the latest Kaggle image - get an error.  So I took the orig notebook: https://www.kaggle.com/code/awsaf49/uwmgi-unet-infer-pytorch/notebook and made a copy.  It uses an image from Oct 2021 and that works.","metadata":{}},{"cell_type":"code","source":"!pip install -q '../input/gitractsegmentationmodels/pretrainedmodels-0.7.4/pretrainedmodels-0.7.4'\n!pip install -q '../input/gitractsegmentationmodels/efficientnet_pytorch-0.6.3/efficientnet_pytorch-0.6.3'\n!pip install -q '../input/gitractsegmentationmodels/timm-0.4.12-py3-none-any.whl'\n!pip install -q '../input/gitractsegmentationmodels/segmentation_models_pytorch-0.2.1-py3-none-any.whl'\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:32:24.331435Z","iopub.execute_input":"2022-06-12T21:32:24.332238Z","iopub.status.idle":"2022-06-12T21:34:23.910526Z","shell.execute_reply.started":"2022-06-12T21:32:24.332142Z","shell.execute_reply":"2022-06-12T21:34:23.909556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q '../input/einops041/einops-0.4.1-py3-none-any.whl'","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:34:23.912734Z","iopub.execute_input":"2022-06-12T21:34:23.913047Z","iopub.status.idle":"2022-06-12T21:34:52.26847Z","shell.execute_reply.started":"2022-06-12T21:34:23.913008Z","shell.execute_reply":"2022-06-12T21:34:52.267525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os, glob\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom einops import rearrange, reduce, repeat\nimport segmentation_models_pytorch as smp\nfrom tqdm import tqdm\n\nfrom torchvision.transforms import PILToTensor\nfrom torchvision import transforms\nfrom torch.nn import functional as F\n\nROOT_FOLDER = '../input/uw-madison-gi-tract-image-segmentation/'\n#ROOT_FOLDER = '/media/SSD/gi-tract/uw-madison-gi-tract-image-segmentation/'\n\nMODEL_FOLDER = '../input/gi-tract-models'\n#MODEL_FOLDER = '/media/SSD/gi-tract/uw-madison-gi-tract-image-segmentation/kaggle models'\n\nmodel_file_base = 'Unet-1-1.pth'\nn_folds = 5\n\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n#DEVICE = \"cpu\"\n#DEVICE = \"cuda:0\"","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.614062,"end_time":"2022-05-16T10:44:53.024015","exception":false,"start_time":"2022-05-16T10:44:52.409953","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-12T21:34:52.270176Z","iopub.execute_input":"2022-06-12T21:34:52.27051Z","iopub.status.idle":"2022-06-12T21:35:00.36146Z","shell.execute_reply.started":"2022-06-12T21:34:52.270469Z","shell.execute_reply":"2022-06-12T21:35:00.36058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"markdown","source":"## Process the Test files","metadata":{}},{"cell_type":"markdown","source":"#### Set the debug mode\n`debug = True` will result in using all the training images to do inference and also do a check on the resize+ pad transform needed to invert the basic training transform  (crop+resize)\n\n\n`debug = False` will result in using the test images","metadata":{}},{"cell_type":"code","source":"sub_df = pd.read_csv(ROOT_FOLDER+'sample_submission.csv')\nprint(\"sub length:\", len(sub_df))\nif not len(sub_df):\n    debug = True\nelse:\n    debug = False\n    \nprint(\"debug:\", debug)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:38:34.956242Z","iopub.execute_input":"2022-06-12T21:38:34.956577Z","iopub.status.idle":"2022-06-12T21:38:34.97118Z","shell.execute_reply.started":"2022-06-12T21:38:34.956542Z","shell.execute_reply":"2022-06-12T21:38:34.970103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if debug:  # Use the training files as input\n    test_fnames = glob.glob(\"{}train/*/*/scans/*png\".format(ROOT_FOLDER))\n    file_df = pd.DataFrame(test_fnames)\n    file_df = file_df[:1000*3]\nelse:  # Use the test files as input\n    test_fnames = glob.glob(\"{}test/*/*/scans/*png\".format(ROOT_FOLDER))\n    file_df = pd.DataFrame(test_fnames)\n    \n    \n","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:38:36.588632Z","iopub.execute_input":"2022-06-12T21:38:36.589248Z","iopub.status.idle":"2022-06-12T21:38:37.215456Z","shell.execute_reply.started":"2022-06-12T21:38:36.589211Z","shell.execute_reply":"2022-06-12T21:38:37.214633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Samples to predict:\",len(file_df))","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:38:49.223125Z","iopub.execute_input":"2022-06-12T21:38:49.223409Z","iopub.status.idle":"2022-06-12T21:38:49.228918Z","shell.execute_reply.started":"2022-06-12T21:38:49.223378Z","shell.execute_reply":"2022-06-12T21:38:49.228045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_df.columns = [\"path\"]","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:38:57.595459Z","iopub.execute_input":"2022-06-12T21:38:57.596136Z","iopub.status.idle":"2022-06-12T21:38:57.602781Z","shell.execute_reply.started":"2022-06-12T21:38:57.596085Z","shell.execute_reply":"2022-06-12T21:38:57.600014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to submit a csv with the following columns\n\n`['id', 'class', 'predicted']`\n","metadata":{}},{"cell_type":"markdown","source":"We need a way to get the `id` for the submission from the file paths\n\nBuild a function that does the string manipulation","metadata":{}},{"cell_type":"code","source":"def id_from_path(p):\n    \n    #p = '/media/SSD/gi-tract/uw-madison-gi-tract-image-segmentation/train/case24/case24_day25/scans/slice_0046_266_266_1.50_1.50.png'\n    s1 = p.split('/')\n    filename = s1[-1] # 'slice_0046_266_266_1.50_1.50.png'\n    case_day_str = s1[-3] # 'case24_day25'\n    slice_str = '_'.join(filename.split('_')[-7:-4]) #'slice_0046'\n    id_str = case_day_str + '_' + slice_str #'case24_day25_slice_0046'\n    return id_str\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:39:25.25933Z","iopub.execute_input":"2022-06-12T21:39:25.259663Z","iopub.status.idle":"2022-06-12T21:39:25.265376Z","shell.execute_reply.started":"2022-06-12T21:39:25.259627Z","shell.execute_reply":"2022-06-12T21:39:25.264524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_df[\"id\"] = file_df[\"path\"].apply(id_from_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:39:25.703383Z","iopub.execute_input":"2022-06-12T21:39:25.704146Z","iopub.status.idle":"2022-06-12T21:39:25.721862Z","shell.execute_reply.started":"2022-06-12T21:39:25.704104Z","shell.execute_reply":"2022-06-12T21:39:25.720539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_df","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:39:28.348339Z","iopub.execute_input":"2022-06-12T21:39:28.349257Z","iopub.status.idle":"2022-06-12T21:39:28.3847Z","shell.execute_reply.started":"2022-06-12T21:39:28.349211Z","shell.execute_reply":"2022-06-12T21:39:28.383761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get the NN Models\n\nFor each fold of the data we have a model, so we need to get all of them.  They are not huge so we can keep all of them in the GPU.","metadata":{}},{"cell_type":"code","source":"models = []\nfor fold in range(0,n_folds):\n    \n    model = smp.Unet(\n    encoder_name=\"efficientnet-b0\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n    encoder_weights=None,     # don't need initialization since we will load our models\n    in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n    classes=3)                    # model output channels (number of classes in your dataset)\n    \n    model_in_str = MODEL_FOLDER+ \"/\" + \"fold-\" + str(fold) + '-' + model_file_base \n    print(model_in_str)\n    model.load_state_dict(torch.load(model_in_str))\n    model.to(torch.device(DEVICE))\n    model.eval()\n    models.append(model)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:39:41.045444Z","iopub.execute_input":"2022-06-12T21:39:41.046054Z","iopub.status.idle":"2022-06-12T21:39:48.842525Z","shell.execute_reply.started":"2022-06-12T21:39:41.046012Z","shell.execute_reply":"2022-06-12T21:39:48.841649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create a Pytorch Dataset for inference\n\nThe main difference relative to training is we don't have the ground truth run length encoded mask.\nWe also provide the `id` used in the submission because its the key","metadata":{"papermill":{"duration":0.055077,"end_time":"2022-05-16T10:47:36.536873","exception":false,"start_time":"2022-05-16T10:47:36.481796","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Dataset_from_df_inference(torch.utils.data.Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n        self.pil_to_tensor = PILToTensor()\n\n        \n    def __len__(self):\n        return self.df.shape[0]\n        \n    \n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        \n        img_path = row.path\n        # Use PIL Image to read the image files since it handles 32 bit images\n        img = self.pil_to_tensor(Image.open(img_path))\n        #print(img.shape)\n        \n        \n        \n        # capture the shape of the original image because we want our final mask\n        # used for RLE to match this shape\n        mask_shape = img.shape\n\n        \n        \n        # resize our image to a consistent size to provide as input to model\n        if self.transform:\n            img = self.transform(img)\n    \n        \n        \n        return {\n            \"image\": img,\n            \"mask_shape\": mask_shape,\n            \"id\": row.id #return the row id used in the submission\n        }\n        \n        \n    \n    \n","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:39:52.518627Z","iopub.execute_input":"2022-06-12T21:39:52.519345Z","iopub.status.idle":"2022-06-12T21:39:52.528143Z","shell.execute_reply.started":"2022-06-12T21:39:52.519303Z","shell.execute_reply":"2022-06-12T21:39:52.527189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define our transform\nWe use the basic imaging sizing from Training because that recreates the conditions under which we trained","metadata":{}},{"cell_type":"code","source":"test_transforms = transforms.Compose(\n    [transforms.CenterCrop((266,266)),\n    transforms.ConvertImageDtype(torch.float32),\n    transforms.Resize((288,288),interpolation=transforms.InterpolationMode.BICUBIC)])  # multiple of 32 for UUnet","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:41:01.645473Z","iopub.execute_input":"2022-06-12T21:41:01.645802Z","iopub.status.idle":"2022-06-12T21:41:01.65119Z","shell.execute_reply.started":"2022-06-12T21:41:01.645769Z","shell.execute_reply":"2022-06-12T21:41:01.649991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create the Dataset\nAnd ensure it returns what we expect","metadata":{}},{"cell_type":"code","source":"test_dataset = Dataset_from_df_inference(file_df,test_transforms)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:41:02.604459Z","iopub.execute_input":"2022-06-12T21:41:02.605251Z","iopub.status.idle":"2022-06-12T21:41:02.60918Z","shell.execute_reply.started":"2022-06-12T21:41:02.60521Z","shell.execute_reply":"2022-06-12T21:41:02.608171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset[0]['mask_shape'], test_dataset[0]['id']","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:41:03.20859Z","iopub.execute_input":"2022-06-12T21:41:03.209163Z","iopub.status.idle":"2022-06-12T21:41:03.272208Z","shell.execute_reply.started":"2022-06-12T21:41:03.209123Z","shell.execute_reply":"2022-06-12T21:41:03.271403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(test_dataset[0]['image'].squeeze().numpy(),cmap='gray')","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:41:06.251066Z","iopub.execute_input":"2022-06-12T21:41:06.251667Z","iopub.status.idle":"2022-06-12T21:41:06.520281Z","shell.execute_reply.started":"2022-06-12T21:41:06.251625Z","shell.execute_reply":"2022-06-12T21:41:06.519481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference Processing","metadata":{}},{"cell_type":"markdown","source":"Create the dataloader. Kaggle reccommends `num_workers=2` when using the GPU.  We may have to change the `batch_size` and `pin_memory` settings for Kaggle virtual machine.","metadata":{}},{"cell_type":"code","source":"test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                               batch_size=16,\n                                               num_workers=2,\n                                               pin_memory=True, #pagelock the memory for faster loads to GPU RAM\n                                               shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:41:11.332613Z","iopub.execute_input":"2022-06-12T21:41:11.333357Z","iopub.status.idle":"2022-06-12T21:41:11.338203Z","shell.execute_reply.started":"2022-06-12T21:41:11.333316Z","shell.execute_reply":"2022-06-12T21:41:11.337238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\ndef rle_encode(img):\n    \"\"\" TBD\n    \n    Args:\n        img (np.array): \n            - 1 indicating mask\n            - 0 indicating background\n    \n    Returns: \n        run length as string formated\n    \"\"\"\n    \n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:41:14.045736Z","iopub.execute_input":"2022-06-12T21:41:14.046608Z","iopub.status.idle":"2022-06-12T21:41:14.053892Z","shell.execute_reply.started":"2022-06-12T21:41:14.046555Z","shell.execute_reply":"2022-06-12T21:41:14.052577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Threshold for turning the masking image prediction into an image with pixels of 0 or 1 per channel\nReflects how high we need the predicted probababilty before we count the pixel in the mask\nAlso the image resizing impacts the mask edges (we use bicubic interpolation to avoid some of this) so a lower threshold might be needed to have the masks with pixels of 0 or 1 register edges better.\n\nShould be between 0 and 1.0, on the higher side.","metadata":{}},{"cell_type":"code","source":"\nthreshold = .8","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:41:15.400571Z","iopub.execute_input":"2022-06-12T21:41:15.401146Z","iopub.status.idle":"2022-06-12T21:41:15.405068Z","shell.execute_reply.started":"2022-06-12T21:41:15.401105Z","shell.execute_reply":"2022-06-12T21:41:15.404218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loop through our test data and create the submission","metadata":{}},{"cell_type":"code","source":"sub = {'id':[], 'class':[], 'predicted':[]} # a dict to store our submission predictions\npbar = tqdm(total=len(test_dataloader))\n\n\nfor batch in test_dataloader:\n    \n    images = batch['image'].to(DEVICE)\n    mask_shape = batch['mask_shape']\n    ids = batch['id']\n    \n\n    #Take each of our fold models and average the outputs together\n    with torch.no_grad():\n        output = models[0](images)\n        output = torch.nn.Sigmoid()(output)/n_folds\n        mask = output\n        for i in range(1,n_folds):\n            output = models[i](images)\n            output = torch.nn.Sigmoid()(output)/n_folds\n            mask = mask + output\n    \n    #print(\"1\", mask.shape)\n\n\n    #From here we need to apply the invert of the the basic image crop and resize exactly to the mask image\n    #and then apply run length encoding to that image\n    \n    mask = transforms.Resize((266,266),interpolation=transforms.InterpolationMode.BICUBIC)(mask) # undo resize to 288x288\n    #print(\"3\", mask.shape)\n    \n    #print(4,mask_shape)\n    \n    a0 = torch.div(mask_shape[1] - 266,2,rounding_mode = 'floor') # (mask_shape[1] - 266)//2\n    a1 = torch.div(mask_shape[2] - 266,2,rounding_mode = 'floor')\n    #print(a0,a1)\n    \n    # Since we are processing a batch, the final mask shape may change within a batch\n    # So we can't store that as a 1 tensor per batch\n    # Need to process each batch item individually from here\n\n    #single_mask = torch.zeros_like(mask[0])\n    batch_size = mask.shape[0]  # need to get for each batch since last batch may smaller\n    for b in range(0,batch_size):\n        single_mask = F.pad(mask[b],(a1[b], a1[b], a0[b], a0[b]),  \"constant\", 0) #padding param order confirmed\n        #print(\"5\", single_mask.shape)\n            \n        single_mask = (single_mask > threshold)*1.0  # Run Length encoding requires a mask with 0 or 1\n        single_mask = single_mask.cpu().detach().numpy() # go from a tensor on the GPU to a numpy on CPU\n\n        # create 3 submission rows, one for each organ\n        sub_id = ids[b] #get the submission id from the batch\n        \n        large_bowel = rle_encode(single_mask[0])\n        sub['id'].append(sub_id)\n        sub['class'].append('large_bowel')\n        sub['predicted'].append(large_bowel)\n\n        small_bowel = rle_encode(single_mask[1])\n        sub['id'].append(sub_id)\n        sub['class'].append('small_bowel')\n        sub['predicted'].append(small_bowel)\n\n        stomach = rle_encode(single_mask[2])\n        sub['id'].append(sub_id)\n        sub['class'].append('stomach')\n        sub['predicted'].append(stomach)    \n\n    \n\n        \n   \n    pbar.update(1)\n    #break\npbar.close()\ntorch.cuda.empty_cache()\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-06-12T21:41:17.158001Z","iopub.execute_input":"2022-06-12T21:41:17.158501Z","iopub.status.idle":"2022-06-12T21:42:11.938234Z","shell.execute_reply.started":"2022-06-12T21:41:17.158461Z","shell.execute_reply":"2022-06-12T21:42:11.937331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df = pd.DataFrame(sub)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:44:31.588827Z","iopub.execute_input":"2022-06-12T21:44:31.589138Z","iopub.status.idle":"2022-06-12T21:44:31.59828Z","shell.execute_reply.started":"2022-06-12T21:44:31.589101Z","shell.execute_reply":"2022-06-12T21:44:31.597272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace NaNs with an empty string for the 'predicted' column\npred_df.predicted = pred_df.predicted.fillna('')","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:44:34.212031Z","iopub.execute_input":"2022-06-12T21:44:34.212574Z","iopub.status.idle":"2022-06-12T21:44:34.222249Z","shell.execute_reply.started":"2022-06-12T21:44:34.212526Z","shell.execute_reply":"2022-06-12T21:44:34.221442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create submission file\n\nWe need to follow the id and class row order of the sample_submission file exactly to get a score","metadata":{}},{"cell_type":"code","source":"if not debug:\n    sub_df = pd.read_csv('../input/uw-madison-gi-tract-image-segmentation/sample_submission.csv')\n    del sub_df['predicted']\n    sub_df = sub_df.merge(pred_df, on=['id','class'])\n    sub_df.to_csv('./submission.csv',index=False)\nelse:\n    sub_df = pred_df.copy()\n    del sub_df['predicted']\n    sub_df = sub_df.merge(pred_df, on=['id','class'])\n    sub_df.to_csv('./submission.csv',index=False)\n    display(sub_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:44:47.511316Z","iopub.execute_input":"2022-06-12T21:44:47.511821Z","iopub.status.idle":"2022-06-12T21:44:47.743502Z","shell.execute_reply.started":"2022-06-12T21:44:47.511773Z","shell.execute_reply":"2022-06-12T21:44:47.742793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:45:27.653223Z","iopub.execute_input":"2022-06-12T21:45:27.653522Z","iopub.status.idle":"2022-06-12T21:45:27.666001Z","shell.execute_reply.started":"2022-06-12T21:45:27.653476Z","shell.execute_reply":"2022-06-12T21:45:27.665233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}